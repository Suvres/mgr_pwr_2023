@article{Chen2021,
   abstract = {Tabular data are ubiquitous in real world applications. Although many
commonly-used neural components (e.g., convolution) and extensible neural
networks (e.g., ResNet) have been developed by the machine learning community,
few of them were effective for tabular data and few designs were adequately
tailored for tabular data structures. In this paper, we propose a novel and
flexible neural component for tabular data, called Abstract Layer (AbstLay),
which learns to explicitly group correlative input features and generate
higher-level features for semantics abstraction. Also, we design a structure
re-parameterization method to compress the learned AbstLay, thus reducing the
computational complexity by a clear margin in the reference phase. A special
basic block is built using AbstLays, and we construct a family of Deep Abstract
Networks (DANets) for tabular data classification and regression by stacking
such blocks. In DANets, a special shortcut path is introduced to fetch
information from raw tabular features, assisting feature interactions across
different levels. Comprehensive experiments on seven real-world tabular
datasets show that our AbstLay and DANets are effective for tabular data
classification and regression, and the computational complexity is superior to
competitive methods. Besides, we evaluate the performance gains of DANet as it
goes deep, verifying the extendibility of our method. Our code is available at
https://github.com/WhatAShot/DANet.},
   author = {Jintai Chen and Kuanlun Liao and Yao Wan and Danny Z. Chen and Jian Wu},
   doi = {10.1609/aaai.v36i4.20309},
   isbn = {1577358767},
   issn = {2159-5399},
   journal = {Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022},
   month = {12},
   pages = {3930-3938},
   publisher = {Association for the Advancement of Artificial Intelligence},
   title = {DANets: Deep Abstract Networks for Tabular Data Classification and Regression},
   volume = {36},
   url = {https://arxiv.org/abs/2112.02962v4},
   year = {2021},
}
@article{sckit-learn,
   author = {F Pedregosa and G Varoquaux and A Gramfort and B Michel V.
and Thirion and O Grisel and M Blondel and R Prettenhofer P.
and Weiss and V Dubourg and J Vanderplas and A Passos and D Cournapeau and M Brucher and M Perrot and E Duchesnay},
   journal = {Journal of Machine Learning Research},
   pages = {2825-2830},
   title = {Scikit-learn: Machine Learning in Python},
   volume = {12},
   year = {2011},
}
@misc{Azure,
   author = {azure},
   title = {Cloud Computing Services | Microsoft Azure},
   url = {https://azure.microsoft.com/en-us/},
}
@misc{Danet,
   author = {danet},
   title = {GitHub - WhatAShot/DANet: DANets (a family of neural networks) for tabular data classification/ regression.},
   url = {https://github.com/WhatAShot/DANet},
}
@article{Blyszcz2022,
   author = {Bartosz Błyszcz},
   city = {Kraków},
   institution = {Akademia Górniczo-Hutnicza im. Stanisława Staszica w Krakowie},
   month = {9},
   title = {Wykorzystanie algorytmów genetycznych w systemach wykrywania intruzów w sieciach komputerowych},
   year = {2022},
}
