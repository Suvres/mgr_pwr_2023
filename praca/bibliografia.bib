@misc{LightGBM,
author = {LightGBM},
title = {{Features — LightGBM 4.0.0 documentation}},
url = {https://lightgbm.readthedocs.io/en/stable/Features.html},
urldate = {2023-09-16}
}
@misc{Suvres2023,
author = {Suvres2023},
title = {{GitHub - Suvres/gnb-gp: comparison of GNB with GNB-GA}},
url = {https://github.com/Suvres/gnb-gp},
urldate = {2023-09-15}
}
@misc{Statsoft,
author = {Statsoft},
title = {{SVMIntro3.gif (obraz GIF, 358×131 pikseli)}},
url = {https://www.statsoft.pl/textbook/graphics/SVMIntro3.gif},
urldate = {2023-09-13}
}
@misc{IBM,
author = {IBM},
title = {{Spos{\'{o}}b dzia{\l}ania algorytmu SVM - IBM Documentation}},
url = {https://www.ibm.com/docs/pl/spss-modeler/saas?topic=models-how-svm-works},
urldate = {2023-09-13}
}
@misc{MicrosoftLearn2023,
author = {{Microsoft Learn}},
title = {{Create compute clusters - Azure Machine Learning | Microsoft Learn}},
url = {https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster?view=azureml-api-2&tabs=python},
urldate = {2023-09-11},
year = {2023}
}
@misc{Ahlashkari2022,
author = {Ahlashkari},
title = {{GitHub - ahlashkari/CICFlowMeter: CICFlowmeter-V4.0 (formerly known as ISCXFlowMeter) is an Ethernet traffic Bi-flow generator and analyzer for anomaly detection that has been used in many Cybersecurity datsets such as Android Adware-General Malware datas}},
url = {https://github.com/ahlashkari/CICFlowMeter},
urldate = {2023-09-11},
year = {2022}
}
@misc{MicrosoftAzuref,
author = {{Microsoft Azure}},
title = {{Azure Machine Learning — uczenie maszynowe jako us{\l}uga}},
url = {https://azure.microsoft.com/pl-pl/products/machine-learning},
urldate = {2023-09-08}
}
@misc{MicrosoftAzured,
author = {{Microsoft Azure}},
title = {{Azure global infrastructure experience}},
url = {https://datacenters.microsoft.com/globe/explore},
urldate = {2023-09-08}
}
@misc{MicrosoftAzuree,
author = {{Microsoft Azure}},
title = {{Azure global infrastructure experience}},
url = {https://datacenters.microsoft.com/globe/explore?info=region_polandcentral},
urldate = {2023-09-08}
}
@misc{Datashift,
author = {Datashift},
title = {{Microsoft Azure}},
url = {https://www.datashift.eu/technology/microsoft-azure?fbclid=IwAR262r9Kdc0oeF1I8PmCmCuu-P6-5VSHnoKfPvjTJTsEmOkIgmVmfSuuIS8},
urldate = {2023-09-08}
}
@misc{Roosevelt2022,
author = {Roosevelt, Abandy},
title = {{History of Microsoft Azure}},
url = {https://techcommunity.microsoft.com/t5/educator-developer-blog/the-history-of-microsoft-azure/ba-p/3574204 https://techcommunity.microsoft.com/t5/educator-developer-blog/the-history-of-microsoft-azure/ba-p/3574204#},
urldate = {2023-09-08},
year = {2022}
}
@misc{Datashifta,
author = {Datashift},
title = {{MS Azure.png}},
url = {https://cdn.nimbu.io/s/znvdo1j/pages/8g7p2fo/MS Azure.png?33zmiw4},
urldate = {2023-09-08}
}
@misc{MicrosoftAzure,
author = {{Microsoft Azure}},
title = {{Mo{\.{z}}liwo{\'{s}}ci chmury na platformie Azure}},
url = {https://azure.microsoft.com/pl-pl/solutions/cloud-enablement/?activetab=pivot:microsoftlearntab},
urldate = {2023-09-06}
}
@misc{MicrosoftAzurea,
author = {{Microsoft Azure}},
title = {{Ekonomia chmury — wskaz{\'{o}}wki dotycz{\c{a}}ce przypadk{\'{o}}w biznesowych w chmurze}},
url = {https://azure.microsoft.com/pl-pl/solutions/cloud-economics/},
urldate = {2023-09-06}
}
@misc{MicrosoftAzureb,
author = {{Microsoft Azure}},
title = {{Infrastruktura globalna}},
url = {https://azure.microsoft.com/pl-pl/explore/global-infrastructure/},
urldate = {2023-09-06}
}
@misc{MicrosoftAzurec,
author = {{Microsoft Azure}},
title = {{Poznaj platform{\c{e}} Azure}},
url = {https://azure.microsoft.com/pl-pl/explore/},
urldate = {2023-09-06}
}
@misc{GoogleAppSheet,
author = {GoogleAppSheet},
title = {{Google AppSheet | Build apps with no code}},
url = {https://about.appsheet.com/home/},
urldate = {2023-09-05}
}
@misc{AmazonQuickSight,
author = {AmazonQuickSight},
title = {{Business Intelligence Service – Amazon QuickSight – AWS}},
url = {https://aws.amazon.com/quicksight/},
urldate = {2023-09-05}
}
@misc{Microsoftc,
author = {Microsoft},
title = {{Co to jest us{\l}uga Power Apps? - Power Apps | Microsoft Learn}},
url = {https://learn.microsoft.com/pl-pl/power-apps/powerapps-overview},
urldate = {2023-09-05}
}
@misc{Microsoft,
author = {Microsoft},
title = {{Om{\'{o}}wienie kart dla us{\l}ugi Power Apps - Power Apps | Microsoft Learn}},
url = {https://learn.microsoft.com/pl-pl/power-apps/cards/overview},
urldate = {2023-09-05}
}
@misc{Microsofta,
author = {Microsoft},
title = {{Om{\'{o}}wienie tworzenia aplikacji opartej na modelu z Power Apps - Power Apps | Microsoft Learn}},
url = {https://learn.microsoft.com/pl-pl/power-apps/maker/model-driven-apps/model-driven-app-overview},
urldate = {2023-09-05}
}
@misc{Microsoftb,
author = {Microsoft},
title = {{Om{\'{o}}wienie tworzenia aplikacji kanw - Power Apps | Microsoft Learn}},
url = {https://learn.microsoft.com/pl-pl/power-apps/maker/canvas-apps/getting-started},
urldate = {2023-09-05}
}
@misc{MsPowerApps2023,
author = {{Microsoft Power Apps}},
title = {{Aplikacje biznesowe}},
url = {https://powerapps.microsoft.com/pl-pl/},
urldate = {2023-09-04},
year = {2023}
}
@article{Bock2021,
author = {Bock, Alexander C. and Frank, Ulrich},
doi = {10.1007/S12599-021-00726-8/FIGURES/1},
file = {:C\:/Users/Bartosz/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bock, Frank - 2021 - Low-Code Platform.pdf:pdf},
issn = {18670202},
journal = {Business and Information Systems Engineering},
keywords = {Citizen developer,Conceptual modeling,Low-code,Organizational agility,Software development environment,Software development productivity},
month = {dec},
number = {6},
pages = {733--740},
publisher = {Springer Gabler},
title = {{Low-Code Platform}},
url = {https://link.springer.com/article/10.1007/s12599-021-00726-8},
volume = {63},
year = {2021}
}
@misc{WordpressDeveloper2023,
author = {Wordpress},
title = {{Playground Demo}},
url = {https://developer.wordpress.org/playground/demo/},
urldate = {2023-09-04},
year = {2023}
}
@misc{Powerapps2023,
author = {Microsoft},
title = {{PowerApps}},
url = {https://guidedtour.microsoft.com/guidedtour/scenarios/power-apps/2.2.png},
urldate = {2023-09-04},
year = {2023}
}
@misc{Wix2023,
abstract = {Create a free website with Wix.com. Customize with Wix' website builder, no coding skills needed. Choose a design, begin customizing and be online today},
author = {Wix},
booktitle = {Wix.com, Inc},
title = {{Free website builder | Create a free website}},
url = {https://www.wix.com/},
urldate = {2023-09-04},
year = {2016}
}
@misc{Joomla2023,
author = {JoomlaORG},
title = {{Joomla! - Content Management System to build websites}},
url = {https://www.joomla.org/ https://www.joomla.org/about-joomla.html},
urldate = {2023-09-04},
year = {2021}
}
@misc{Wordpress2023,
author = {Wordpress},
booktitle = {Wordpress},
title = {{WordPress.com: Build a Site, Sell Your Stuff}},
url = {https://wordpress.com/},
urldate = {2023-09-04},
year = {2023}
}
@article{Hirzel2022,
abstract = {Traditionally, computer programming has been the prerogative of professional developers using textual programming languages such as C, Java, or Python. Low-code programming promises an alternative: letting citizen developers create programs using visual abstractions, demonstrations, or natural language. While low-code programming is currently getting a lot of attention in industry, the relevant research literature is scattered, and in fact, rarely uses the term "low-code". This article brings together low-code literature from various research fields, explaining how techniques work while providing a unified point of view. Low-code has the potential to empower more people to automate tasks by creating computer programs, making them more productive and less dependent on scarce professional software developers.},
archivePrefix = {arXiv},
arxivId = {2205.02282},
author = {Hirzel, Martin},
eprint = {2205.02282},
file = {:C\:/Users/Bartosz/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hirzel - 2022 - Low-Code Programming Models.pdf:pdf},
month = {may},
title = {{Low-Code Programming Models}},
url = {https://arxiv.org/abs/2205.02282v1 http://arxiv.org/abs/2205.02282},
year = {2022}
}
@article{Rokis2022,
abstract = {Low-code/no-code software development is an emerging approach delivering the opportunity to build software with a minimal need for manual coding and enhancing the involvement of non-programmers in software development. Low-code principles allow enterprises to save time and costs through a more rapid development pace and to improve software products quality by bringing closer together business and information technologies as well as promoting automation. Nevertheless, the low-code/no-code approach is a relatively new and continuously progressing domain that requires understanding of existing challenges and identification of improvement directions. In this paper, challenges in the low-code software development process and suggestions for their mitigation are identified and amalgamated with the purpose to deliver insights into the current state of the low-code/no-code development process and identify areas for further research and development.},
author = {Rokis, Karlis and Kirikova, Marite},
doi = {10.1007/978-3-031-16947-2_1/COVER},
isbn = {9783031169465},
issn = {18651356},
journal = {Lecture Notes in Business Information Processing},
keywords = {Citizen developer,Low-code,Low-code development platform,No-code,Requirements,Software development},
pages = {3--17},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Challenges of Low-Code/No-Code Software Development: A Literature Review}},
url = {https://link.springer.com/chapter/10.1007/978-3-031-16947-2_1},
volume = {462 LNBIP},
year = {2022}
}
@misc{Algolytics,
author = {Algolytics},
title = {{Jak oceni{\'{c}} jako{\'{s}}{\'{c}} i poprawno{\'{s}}{\'{c}} modeli klasyfikacyjnych ? Cz{\c{e}}{\'{s}}{\'{c}} 4- Krzywa ROC}},
url = {https://algolytics.pl/tutorial-jak-ocenic-jakosc-i-poprawnosc-modeli-klasyfikacyjnych-czesc-4-krzywa-roc/},
urldate = {2023-09-04}
}
@misc{MicrosoftDeep2023,
abstract = {Este art{\'{i}}culo explica el aprendizaje profundo frente al aprendizaje autom{\'{a}}tico y c{\'{o}}mo encajan en la categor{\'{i}}a m{\'{a}}s amplia de inteligencia artificial. Obtenga informaci{\'{o}}n sobre las soluciones de aprendizaje profundo que puede desarrollar en Azure Machine Learning, como la detecci{\'{o}}n de fraudes, el reconocimiento de voz y facial, el an{\'{a}}lisis de sentimientos y la previsi{\'{o}}n de series de tiempo. Para obtener orientaci{\'{o}}n sobre c{\'{o}}mo elegir algoritmos para sus soluciones, consulte la Hoja de referencia de algoritmos de aprendizaje autom{\'{a}}tico .},
author = {Microsoft},
booktitle = {Microsoft Docs},
title = {{Deep learning vs. machine learning - Azure Machine Learning}},
url = {https://learn.microsoft.com/en-us/azure/machine-learning/concept-deep-learning-vs-machine-learning?view=azureml-api-2},
urldate = {2023-09-02},
year = {2022}
}
@misc{Koch2022,
author = {Koch, Robert},
title = {{History of Machine Learning – A Journey through the Timeline}},
url = {https://www.clickworker.com/customer-blog/history-of-machine-learning/},
urldate = {2023-09-02},
year = {2022}
}
@inproceedings{Fradkov2020,
abstract = {Machine learning belongs to the crossroad of cybernetics (control science) and computer science. It is attracting recently an overwhelming interest, both of professionals and of the general public. In the talk a brief overview of the historical development of the machine learning field with a focus on the development of mathematical apparatus in its first decades is provided. A number of little-known facts published in hard to reach sources are presented.},
author = {Fradkov, Alexander L.},
booktitle = {IFAC-PapersOnLine},
doi = {10.1016/j.ifacol.2020.12.1888},
file = {:C\:/Users/Bartosz/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fradkov - 2020 - Early History of Machine Learning.pdf:pdf},
issn = {24058963},
keywords = {Convex optimization,Machine learning,Neural networks,Separation theorems},
month = {jan},
number = {2},
pages = {1385--1390},
publisher = {Elsevier},
title = {{Early history of machine learning}},
volume = {53},
year = {2020}
}
@article{BartosSOM,
abstract = {Streszczenie: Artyku{\l} prezentuje sie{\'{c}} SOM jako przyk{\l}ad sieci samoorganizuj{\c{a}}cej si{\c{e}}. Za-wiera podstawowe informacje o tej sieci: jej genez{\c{e}}, struktur{\c{e}}, spos{\'{o}}b funkcjonowania i metod{\c{e}} uczenia (uczenie nienadzorowane z konkurencj{\c{a}}). Ponadto autorka opisuje proces weryfikacji samoorganizuj{\c{a}}cych si{\c{e}} map cech oraz algorytm Kohonena wykorzystywany podczas trenowania sieci SOM. S{\l}owa kluczowe: sie{\'{c}} neuronowa SOM, algorytm Kohonena, sie{\'{c}} samoorganizuj{\c{a}}ca si{\c{e}}.},
author = {Bartos, Karolina},
file = {:C\:/Users/Bartosz/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bartos - 2012 - SIE{\'{C}} SOM JAKO PRZYK{\L}AD SIECI SAMOORGANIZUJ{\c{A}}CEJ SI{\c{E}}.pdf:pdf},
issn = {1507-3866},
keywords = {Kohonen's algorithm,SOM,algorytm Kohonena,self-organizing neural network,sie{\'{c}} neuronowa SOM,sie{\'{c}} samoorganizuj{\c{a}}ca si{\c{e}}},
title = {{SIE{\'{C}} SOM JAKO PRZYK{\L}AD SIECI SAMOORGANIZUJ{\c{A}}CEJ SI{\c{E}}}},
year = {2012}
}
@misc{IBMNetwork,
author = {Pollard, Austin},
booktitle = {Sensor Review},
doi = {10.1108/eb007822},
issn = {02602288},
number = {3},
pages = {115--116},
title = {{What are neural networks?}},
url = {https://www.ibm.com/topics/neural-networks},
volume = {10},
year = {1990}
}
@article{semiLinkedin,
abstract = {Semi-supervised learning has attracted significant attention in pattern recognition and machine learning. Among these methods, a very popular type is semi-supervised support vector machines. However, parameter selection in heat kernel function during the learning process is troublesome and harms the performance improvement of the hypothesis. To solve this problem, a novel local behavioral searching strategy is proposed for semi-supervised learning in this paper. In detail, based on human behavioral learning theory, the support vector machine is regularized with the un-normalized graph Laplacian. After building local distribution of feature space, local behavioral paradigm considers the form of the underlying probability distribution in the neighborhood of a point. Validation of the proposed method is performed with toy and real-life data sets. Results demonstrate that compared with traditional method, our method can more effectively and stably enhance the learning performance.},
author = {Zhang, Chun and Wang, Shafei and Li, Dongsheng and Yang, Junan and Zhang, Jiyang},
doi = {10.1016/j.ijleo.2015.10.089},
issn = {00304026},
journal = {Optik},
keywords = {Behavioral learning,Manifold learning,Regularization,Semi-supervised learning,Support vector machines},
number = {1},
pages = {376--382},
title = {{Semi-supervised behavioral learning and its application}},
url = {https://www.linkedin.com/pulse/semi-supervised-learning-its-application-crafsol-technology/},
volume = {127},
year = {2016}
}
@book{AiScience,
booktitle = {Artificial Intelligence in Science},
doi = {10.1787/a8d820bd-en},
month = {jun},
publisher = {OECD},
title = {{Artificial Intelligence in Science}},
year = {2023}
}
@article{Mahesh2018,
abstract = {Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without being explicitly programmed. Learning algorithms in many applications that's we make use of daily. Every time a web search engine like Google is used to search the internet, one of the reasons that work so well is because a learning algorithm that has learned how to rank web pages. These algorithms are used for various purposes like data mining, image processing, predictive analytics, etc. to name a few. The main advantage of using machine learning is that, once an algorithm learns what to do with data, it can do its work automatically. In this paper, a brief review and future prospect of the vast applications of machine learning algorithms has been made.},
author = {Mahesh, Batta},
doi = {10.21275/ART20203995},
file = {:C\:/Users/Bartosz/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahesh - 2018 - Machine Learning Algorithms-A Review.pdf:pdf},
issn = {2319-7064},
journal = {International Journal of Science and Research},
keywords = {Algorithm,Machine Learning,Pseudo Code,Reinforcement learning,Supervised learning,Unsupervised learning},
title = {{Machine Learning Algorithms-A Review}},
url = {https://www.researchgate.net/publication/344717762},
year = {2018}
}
@article{LinkedInSi,
abstract = {Making Sales A Science},
author = {Pati, Satavisa},
journal = {Emerj},
number = {Ml},
pages = {3--8},
title = {{The Difference Between Artificial Intelligence and Machine Learning}},
url = {https://www.linkedin.com/feed/update/urn:li:activity:7014145513159569408/ https://www.analyticsinsight.net/the-difference-between-artificial-intelligence-and-machine-learning/},
year = {2018}
}
@incollection{OxfordJuly2023,
author = {{Oxford University Press}},
booktitle = {Oxford English Dictionary},
doi = {10.1093/OED/3757635879},
month = {jul},
title = {intelligence, n., sense 1},
year = {2023}
}
@incollection{Asadollahfardi2015,
abstract = {The long course of evolution has given the human brain many desirable characteristics not present in Von Neumann or modern parallel computers. These include massive parallelism, distributed representation and computation, learning ability, generalization ability,adaptivity, inherent contextual information processing, fault tolerance, and low energy consumption. It is hoped that devices based on biological neural networks will possess some of these desirable characteristics.On this basic we come out with the concept of artificial neural network. An artificial neural network, often just called a neural network, is a mathematical model inspired by biological neural networks. A neural network consists of an interconnected group of artificial neurons, and it processes information using a connectionist approach to computation. Neural networks have emerged in the past few years as an area of unusual opportunity for research, development and application to a variety of real world problems. Indeed, neural networks exhibit characteristics and capabilities not provided by any other technology. The article discusses the motivations behind the development of ANNs and describes the basic biological neuron. This paper presents a brief tutorial on artificial neural networks, some of the most commonly used ANN models and briefly describes several applications of it.},
address = {Boston, MA},
author = {Asadollahfardi, Gholamreza},
booktitle = {Interdisciplinary Computing in Java Programming},
doi = {10.1007/978-3-662-44725-3_5},
pages = {77--91},
publisher = {Springer, Boston, MA},
title = {{Artificial Neural Network}},
url = {https://link.springer.com/chapter/10.1007/978-1-4615-0377-4_5},
year = {2015}
}
@misc{azureml,
author = {Microsoft},
title = {{Microsoft Machine Learning Studio (classic)}},
url = {https://studio.azureml.net/},
urldate = {2023-09-01},
year = {2022}
}
@inproceedings{McKinney2010,
abstract = {In this paper we are concerned with the practical issues of working with data sets common to finance, statistics, and other related fields. pandas is a new library which aims to facilitate working with these data sets and to provide a set of fundamental building blocks for implementing statistical models. We will discuss specific design issues encountered in the course of developing pandas with relevant examples and some comparisons with the R language. We conclude by discussing possible future directions for statistical computing and data analysis using Python.},
author = {McKinney, Wes},
booktitle = {Proceedings of the 9th Python in Science Conference},
doi = {10.25080/majora-92bf1922-00a},
pages = {56--61},
title = {{Data Structures for Statistical Computing in Python}},
year = {2010}
}
@misc{Harris2020,
abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
archivePrefix = {arXiv},
arxivId = {2006.10256},
author = {Harris, Charles R and Millman, K Jarrod and van der Walt, St{\'{e}}fan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H and Brett, Matthew and Haldane, Allan and del R{\'{i}}o, Jaime Fern{\'{a}}ndez and Wiebe, Mark and Peterson, Pearu and G{\'{e}}rard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E},
booktitle = {Nature},
doi = {10.1038/s41586-020-2649-2},
eprint = {2006.10256},
issn = {14764687},
month = {sep},
number = {7825},
pages = {357--362},
pmid = {32939066},
publisher = {Springer Science and Business Media LLC},
title = {{Array programming with NumPy}},
url = {https://doi.org/9.1038/s41586-020-2649-2},
volume = {585},
year = {2020}
}
@misc{pandas,
author = {Pandas development team, The},
doi = {9.5281/zenodo.3509134},
month = {feb},
publisher = {Zenodo},
title = {{pandas-dev/pandas: Pandas}},
url = {https://doi.org/9.5281/zenodo.3509134},
year = {2019}
}
@misc{cicds2017kaggle,
author = {UNB},
title = {{CICIDS2017 | Kaggle}},
url = {https://www.kaggle.com/datasets/cicdataset/cicids2017},
urldate = {2023-09-01}
}
@misc{unbkaggle,
abstract = {The CICIDS2017 dataset consists of labeled network flows, including full packet payloads in pcap format, the corresponding profiles and the labeled flows (GeneratedLabelledFlows.zip) and CSV files for machine and deep learning purpose (MachineLearningCSV.zip) are publicly available for researchers. If you are using our dataset, you should cite our related paper which outlining the details of the dataset and its underlying principles:},
author = {Sharafaldin, Iman and Lashkai, Arash Habibi and Ghorbani, Ali A},
booktitle = {Canadian Institute for Cybersecurity},
pages = {1},
title = {{IDS 2017 | Datasets | Research | Canadian Institute for Cybersecurity | UNB}},
url = {https://www.unb.ca/cic/datasets/ids-2017.html},
urldate = {2023-09-01},
year = {2018}
}
@phdthesis{Blyszcz2022,
address = {Krak{\'{o}}w},
author = {{Bartosz B{\l}yszcz}},
month = {sep},
school = {Akademia G{\'{o}}rniczo-Hutnicza im. Stanis{\l}awa Staszica w Krakowie},
title = {{Wykorzystanie algorytm{\'{o}}w genetycznych w systemach wykrywania intruz{\'{o}}w w sieciach komputerowych}},
type = {Praca In{\.{z}}ynierska},
year = {2022}
}
@misc{Danet,
author = {Danet},
title = {{GitHub - WhatAShot/DANet: DANets (a family of neural networks) for tabular data classification/ regression.}},
url = {https://github.com/WhatAShot/DANet},
urldate = {2023-09-01}
}
@misc{Azure,
abstract = {Invent with purpose, realize cost savings, and make your organization more efficient with Microsoft Azure's open and flexible cloud computing platform.},
author = {Microsoft},
booktitle = {Microsoft Azure},
title = {{Cloud Computing Services | Microsoft Azure}},
url = {https://azure.microsoft.com/en-us/},
year = {2020}
}
@article{PedregosaF;VaroquauxG;GramfortA;MichelV;ThirionB;andGriselO.andBlondel.andPrettenhofer2011,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {{Pedregosa, F; Varoquaux, G; Gramfort, A; Michel, V; Thirion, B; and Grisel, O. and Blondel, . and Prettenhofer}, P. and and {and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos}, A. and {Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay}, E. and Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
eprint = {1201.0490},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=2078195%5Cnhttp://arxiv.org/abs/1201.0490},
volume = {12},
year = {2011}
}
@inproceedings{Chen2022,
abstract = {Tabular data are ubiquitous in real world applications. Although many commonly-used neural components (e.g., convolution) and extensible neural networks (e.g., ResNet) have been developed by the machine learning community, few of them were effective for tabular data and few designs were adequately tailored for tabular data structures. In this paper, we propose a novel and flexible neural component for tabular data, called Abstract Layer (ABSTLAY), which learns to explicitly group correlative input features and generate higher-level features for semantics abstraction. Also, we design a structure re-parameterization method to compress the trained ABSTLAY, thus reducing the computational complexity by a clear margin in the reference phase. A special basic block is built using ABSTLAYs, and we construct a family of Deep Abstract Networks (DANETs) for tabular data classification and regression by stacking such blocks. In DANETs, a special shortcut path is introduced to fetch information from raw tabular features, assisting feature interactions across different levels. Comprehensive experiments on seven real-world tabular datasets show that our ABSTLAY and DANETs are effective for tabular data classification and regression, and the computational complexity is superior to competitive methods. Besides, we evaluate the performance gains of DANET as it goes deep, verifying the extendibility of our method. Our code is available at https://github.com/WhatAShot/DANet.},
archivePrefix = {arXiv},
arxivId = {2112.02962},
author = {Chen, Jintai and Liao, Kuanlun and Wan, Yao and Chen, Danny Z. and Wu, Jian},
booktitle = {Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022},
doi = {10.1609/aaai.v36i4.20309},
eprint = {2112.02962},
file = {:C\:/Users/Bartosz/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2021 - DANets Deep Abstract Networks for Tabular Data Classification and Regression.pdf:pdf},
isbn = {1577358767},
issn = {2159-5399},
month = {dec},
pages = {3930--3938},
publisher = {Association for the Advancement of Artificial Intelligence},
title = {{DANETs: Deep Abstract Networks for Tabular Data Classification and Regression}},
url = {https://arxiv.org/abs/2112.02962v4},
volume = {36},
year = {2022}
}
@misc{Google,
author = {Google},
title = {{Lasy decyzyjne | Machine Learning | Google for Developers}},
url = {https://developers.google.com/machine-learning/decision-forests/intro-to-decision-forests-real?hl=pl},
urldate = {2023-09-16}
}
@article{Wang2003,
    abstract = {Inspired by the sophisticated functionality of human brains where hundreds of billions of interconnected neurons process information in parallel, researchers have successfully tried demonstrating certain levels of intelligence on silicon. Examples include language...},
    author = {Sun-Chong Wang},
    city = {Boston, MA},
    doi = {10.1007/978-1-4615-0377-4_5},
    journal = {Interdisciplinary Computing in Java Programming},
    pages = {81-100},
    publisher = {Springer, Boston, MA},
    title = {Artificial Neural Network},
    url = {https://link.springer.com/chapter/10.1007/978-1-4615-0377-4_5},
    year = {2003},
}
@article{Harris2019,
    author = {Charles R Harris and K Jarrod Millman and Stéfan J.
van der Walt and Ralf Gommers and Pauli Virtanen and David
Cournapeau and Eric Wieser and Julian Taylor and Sebastian
Berg and Nathaniel J Smith and Robert Kern and Matti Picus
and Stephan Hoyer and Marten H van Kerkwijk and Matthew
Brett and Allan Haldane and Jaime Fernández del
Río and Mark Wiebe and Pearu Peterson and Pierre
Gérard-Marchant and Kevin Sheppard and Tyler Reddy and Warren Weckesser and Hameer Abbasi and Christoph Gohlke and Travis E Oliphant},
    doi = {9.1038/s41586-020-2649-2},
    issue = {7824},
    journal = {Nature},
    month = {9},
    pages = {356-362},
    publisher = {Springer Science and Business Media LLC},
    title = {Array programming with NumPy},
    volume = {584},
    url = {https://doi.org/9.1038/s41586-020-2649-2},
    year = {2019},
}
@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}
@book{Kusiak2021,
    author = {Jan Kusiak and Anna Danielewska-Tulecka and Piotr Oprocha},
    isbn = {9788301159610},
    publisher = {Wydawnictwo Naukowe PWN SA},
    title = {Optymalizacja},
    year = {2021},
}
@incollection{Sastry2005,
    abstract = {Genetic algorithms (GAs) are search methods based on principles of natural selection and genetics (Fraser, 1957; Bremermann, 1958; Holland, 1975). We start with a brief introduction to simple genetic algorithms and associated terminology. {\textcopyright} 2005 Springer Science+Business Media, LLC.},
    author = {Sastry, Kumara and Goldberg, David and Kendall, Graham},
    booktitle = {Search Methodologies: Introductory Tutorials in Optimization and Decision Support Techniques},
    doi = {10.1007/0-387-28356-0_4},
    isbn = {0387234608},
    pages = {97--125},
    publisher = {Springer, Boston, MA},
    title = {{Genetic algorithms}},
    url = {https://link.springer.com/chapter/10.1007/0-387-28356-0{\_}4},
    year = {2005}
}
@incollection{Koza2005,
    abstract = {The goal of getting computers to automatically solve problems is central to artificial intelligence, machine learning, and the broad area encompassed by what Turing called machine intelligence., (Turing, 1948, 1950). In his talk entitled AI: Where It Has Been and Where It Is Going, machine learning pioneer Arthur Samuel stated the main goal of the fields of machine learning and artificial intelligence: [T]he aim [is]. to get machines to exhibit behavior, which if done by humans, would be assumed to involve the use of intelligence. (Samuel, 1983) Genetic programming is a systematic method for getting computers to automatically solve a problem starting from a high-level statement of what needs to be done. Genetic programming is a domain-independent method that genetically breeds a population of computer programs to solve a problem. Specifically, genetic programming iteratively transforms a population of computer programs into a new generation of programs by applying analogs of naturally occurring genetic operations. This process is illustrated in Figure 5.1. {\textcopyright} 2005 Springer Science+Business Media, LLC.},
    author = {Koza, John R. and Poli, Riccardo},
    booktitle = {Search Methodologies: Introductory Tutorials in Optimization and Decision Support Techniques},
    doi = {10.1007/0-387-28356-0_5},
    isbn = {0387234608},
    pages = {127--164},
    publisher = {Springer, Boston, MA},
    title = {{Genetic programming}},
    url = {https://link.springer.com/chapter/10.1007/0-387-28356-0{\_}5},
    year = {2005}
}
@misc{Joyce2003,
    author = {Joyce and James},
    booktitle = {Zalta, Edward N. (ed.), The Stanford Encyclopedia of Philosophy (Spring 2019 ed.), Metaphysics Research Lab, Stanford University},
    title = {{Bayes' Theorem (Stanford Encyclopedia of Philosophy/Spring 2019 Edition)}},
    url = {https://plato.stanford.edu/archives/spr2019/entries/bayes-theorem/},
    urldate = {2022-05-12},
    year = {2003}
}
@misc{Leung2007,
    abstract = {A statistical classifier called Naive Bayesian classifier is discussed. This classifier is based on the Bayes' Theorem and the maximum posteriori hypothesis. The naive assumption of class conditional independence is often made to reduce the computational cost.},
    author = {Leung, K Ming},
    booktitle = {Polytechnic University Department of Computer Science/Finance and Risk Engineering},
    file = {:home/suvres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leung - 2007 - Naive Bayesian Classifier.pdf:pdf},
    pages = {Lecture Notes},
    title = {{Naive bayesian classifier}},
    url = {http://cis.poly.edu/{~}mleung/FRE7851/f07/naiveBayesianClassifier.pdf},
    urldate = {2022-05-12},
    year = {2007}
}
@misc{Crabtree,
author = {Crabtree, Matt},
title = {{Machine Learning (ML) vs Deep Learning (DL): A Comparative Guide | DataCamp}},
url = {https://www.datacamp.com/tutorial/machine-deep-learning},
urldate = {2024-03-16}
}
@misc{Agrawal2024,
author = {Agrawal, Sumeet Kumar},
booktitle = {Analytics Vidhya},
title = {{Evaluation Metrics For Classification Model - Analytics Vidhya}},
url = {https://www.analyticsvidhya.com/blog/2021/07/metrics-to-evaluate-your-classification-model-to-take-the-right-decisions/},
urldate = {2024-03-16},
year = {2024}
}
